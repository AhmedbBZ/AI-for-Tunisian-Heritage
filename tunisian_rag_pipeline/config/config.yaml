# Tunisian Heritage RAG Pipeline Configuration
# =============================================

# Project paths
paths:
  data_dir: "../tunisian_heritage_data"
  vector_db_dir: "./vector_db"
  models_dir: "./models"
  logs_dir: "./logs"
  cache_dir: "./cache"

# Data ingestion settings
ingestion:
  supported_formats:
    - ".txt"
    - ".json"
    - ".pdf"
    - ".csv"
    - ".md"
  encoding: "utf-8"
  max_file_size_mb: 100

# Text preprocessing
preprocessing:
  min_text_length: 50
  max_text_length: 100000
  remove_extra_whitespace: true
  normalize_unicode: true
  detect_language: true

# Chunking settings
chunking:
  strategy: "semantic"  # Options: fixed, paragraph, semantic, sentence
  chunk_size: 512
  chunk_overlap: 50
  min_chunk_size: 100
  max_chunk_size: 1000
  respect_sentence_boundaries: true
  
# Embedding settings
embeddings:
  model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  # Alternative models:
  # - "sentence-transformers/all-MiniLM-L6-v2" (English only, faster)
  # - "intfloat/multilingual-e5-large" (better quality, slower)
  batch_size: 32
  max_seq_length: 512
  normalize_embeddings: true
  device: "auto"  # auto, cuda, cpu

# Vector database settings
vector_db:
  type: "chromadb"  # Options: chromadb, faiss
  collection_name: "tunisian_heritage"
  distance_metric: "cosine"  # Options: cosine, l2, ip
  persist: true

# Retrieval settings
retrieval:
  top_k: 5
  min_score: 0.3
  rerank: true
  rerank_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  use_mmr: true  # Maximal Marginal Relevance for diversity
  mmr_lambda: 0.7

# LLM settings
llm:
  provider: "huggingface"  # Options: huggingface, openai, ollama
  model_name: "microsoft/Phi-3-mini-4k-instruct"
  # Alternative models:
  # - "mistralai/Mistral-7B-Instruct-v0.2"
  # - "meta-llama/Llama-2-7b-chat-hf"
  # - "gpt-3.5-turbo" (if using OpenAI)
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  do_sample: true
  device: "auto"
  load_in_4bit: true  # Quantization for memory efficiency
  
# Intent classification
intent:
  enabled: true
  categories:
    - factual_question
    - historical_event
    - person_info
    - cultural_topic
    - comparison
    - summary_request
    - general_chat

# Answer generation
generation:
  include_sources: true
  include_confidence: true
  max_context_length: 3000
  answer_format: "detailed"  # Options: brief, detailed, academic

# Fine-tuning settings
fine_tuning:
  enabled: false
  method: "lora"  # Options: lora, qlora, full
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  learning_rate: 2e-4
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4

# Logging settings
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file_logging: true
  console_logging: true

# Performance settings
performance:
  use_gpu: true
  mixed_precision: true
  num_workers: 4
  prefetch_factor: 2
