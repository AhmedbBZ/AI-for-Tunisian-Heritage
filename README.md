tunisian_rag_pipeline/
# Tunisian Heritage RAG Pipeline

A production-ready Retrieval-Augmented Generation (RAG) system for question-answering over Tunisian heritage data, leveraging a local Large Language Model (LLM) for private, high-quality responses.

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ scrapper.py            # Web scraper for data collection
â”œâ”€â”€ Ai_Promptes_Caps/      # AI prompt capts
â”œâ”€â”€ Architecture/          # Architecture documentation
â”œâ”€â”€ tunisian_heritage_data/        # Heritage datasets
â”‚   â”œâ”€â”€ dataset_index.json
â”‚   â”œâ”€â”€ metadata/          # Metadata JSON files
â”‚   â”œâ”€â”€ pdfs/              # PDF documents
â”‚   â”œâ”€â”€ raw_html/          # Raw HTML files
â”‚   â””â”€â”€ texts/             # Text documents
â”œâ”€â”€ tunisian_rag_pipeline/ # Main RAG pipeline
â”‚   â”œâ”€â”€ chat.py            # Interactive chat interface
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ test_retrieval.py   # Retrieval tests
â”‚   â”œâ”€â”€ config/            # Configuration files
â”‚   â”œâ”€â”€ scripts/           # Utility scripts (build, query, fine-tune, diagnose)
â”‚   â”œâ”€â”€ src/               # Source code
â”‚   â”‚   â”œâ”€â”€ data/          # Data processing (chunking, ingestion, preprocessing)
â”‚   â”‚   â”œâ”€â”€ embeddings/    # Embedding models
â”‚   â”‚   â”œâ”€â”€ llm/           # LLM generation and prompts
â”‚   â”‚   â”œâ”€â”€ pipeline/      # RAG pipeline and intent detection
â”‚   â”‚   â”œâ”€â”€ retrieval/     # Vector store and retriever
â”‚   â”‚   â””â”€â”€ utils/         # Helper utilities
â”‚   â”œâ”€â”€ tests/             # Unit tests
â”‚   â””â”€â”€ vector_db/         # ChromaDB persistent storage
â””â”€â”€ vector_db/             # Alternative vector database location
```

## ğŸš€ Quick Start(first u shood download an LLM Localy )

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Build the Vector Database
```bash
python scripts/build_vector_db.py
```

### 3. Query the System
```bash
python scripts/query.py "What caused the Tunisian revolution?"
```

### 4. Interactive Chat
```bash
python chat.py
```

## ğŸ“Š Features

- **Local LLM**: All answers are generated using a local Large Language Model for speed (no cloud required)
- **Multi-language Support**: English, French, Arabic
- **Intelligent Chunking**: Paragraph-aware, topic-aware splitting
- **Semantic Search**: Vector similarity with metadata filtering
- **Source Attribution**: Every answer includes supporting sources
- **Confidence Scoring**: Reliability indicators for answers
- **GPU Optimization**: Batched processing, 8-16GB VRAM friendly

## âš™ï¸ Configuration

Edit `config/config.yaml` to customize:
- Embedding model selection
- Chunk sizes and overlap
- LLM parameters
- Retrieval settings

## ğŸ“– Adding New Data

1. Place new files in `data/` folder
2. Run: `python scripts/build_vector_db.py --update`
3. The system will process only new documents

## ğŸ§ª Testing

```bash
pytest tests/ -v
```

## ğŸ“ˆ Diagnostics

```bash
python scripts/diagnose.py --full
```

## ğŸ¤– Chat Interface

A simple command-line chat interface is provided via `chat.py` for interactive Q&A. All responses are generated locally, ensuring data privacy and low latency.

## License

MIT License
- **Source Attribution**: Every answer includes supporting sources
