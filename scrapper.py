"""
Tunisian Heritage Data Collection Script -- IMPROVED VERSION
============================================================
This script downloads and processes Tunisian heritage texts from open sources.

IMPORTANT: Install required libraries first:
pip install requests beautifulsoup4 PyPDF2 lxml
"""

import requests
from bs4 import BeautifulSoup
import os
import json
import time
from pathlib import Path
import PyPDF2
from urllib.parse import quote
import re

# ==========================================
# CONFIGURATION
# ==========================================

# Create data directory
DATA_DIR = Path("tunisian_heritage_data")
DATA_DIR.mkdir(exist_ok=True)

# Subdirectories
(DATA_DIR / "pdfs").mkdir(exist_ok=True)
(DATA_DIR / "texts").mkdir(exist_ok=True)
(DATA_DIR / "metadata").mkdir(exist_ok=True)
(DATA_DIR / "raw_html").mkdir(exist_ok=True)

# Headers to avoid being blocked
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9'
}

# Project Gutenberg - Public domain books
GUTENBERG_BOOKS = [
    {
        "id": "gutenberg_barbary_states",
        "title": "The Barbary States - North Africa",
        "url": "https://www.gutenberg.org/cache/epub/37703/pg37703.txt",
        "metadata": {"type": "historical", "era": "colonial", "language": "EN"}
    },
    {
        "id": "gutenberg_mediterranean",
        "title": "The Mediterranean - Its Role in History",
        "url": "https://www.gutenberg.org/files/63849/63849-0.txt",
        "metadata": {"type": "historical", "era": "ancient_modern", "language": "EN"}
    }
]

# Internet Archive items
ARCHIVE_ORG_TEXTS = [
    "behindcloseddoor0000heja",
    "historyofmodernt0000perk", 
    "tunisiacrossroad0000perk",
    "tunis1920mcgi",
    "tunisianationali0000ande",
]

# ==========================================
# HELPER FUNCTIONS
# ==========================================

def download_text_from_url(url, use_headers=True):
    """Download text content from URL"""
    try:
        headers = HEADERS if use_headers else {}
        response = requests.get(url, timeout=30, headers=headers, allow_redirects=True)
        response.raise_for_status()
        
        # Try to detect encoding
        if response.encoding:
            return response.text
        else:
            return response.content.decode('utf-8', errors='ignore')
    
    except Exception as e:
        print(f"  âœ— Error: {str(e)[:100]}")
        return None


def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF file"""
    print(f"Extracting text from: {pdf_path.name if hasattr(pdf_path, 'name') else pdf_path}")
    try:
        text = ""
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            num_pages = len(pdf_reader.pages)
            
            for page_num in range(min(num_pages, 50)):  # First 50 pages only
                page = pdf_reader.pages[page_num]
                text += page.extract_text() + "\n\n"
                
                if (page_num + 1) % 10 == 0:
                    print(f"  Processed {page_num + 1}/{num_pages} pages")
        
        print(f"  âœ“ Extracted {len(text)} characters")
        return text
    
    except Exception as e:
        print(f"  âœ— Error extracting text: {str(e)}")
        return ""


def save_metadata(book_id, metadata):
    """Save metadata as JSON"""
    metadata_file = DATA_DIR / "metadata" / f"{book_id}.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"  âœ“ Saved metadata: {metadata_file.name}")


def clean_text(text):
    """Basic text cleaning"""
    # Remove extra whitespace
    text = re.sub(r'\n\n+', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    return text.strip()


def clean_gutenberg_text(text):
    """Remove Gutenberg header and footer"""
    # Remove Gutenberg header
    start_markers = ["*** START OF", "***START OF"]
    for marker in start_markers:
        if marker in text:
            text = text.split(marker, 1)[1]
            break
    
    # Remove Gutenberg footer
    end_markers = ["*** END OF", "***END OF"]
    for marker in end_markers:
        if marker in text:
            text = text.split(marker, 1)[0]
            break
    
    return text.strip()


# ==========================================
# DATA COLLECTION FUNCTIONS
# ==========================================

def download_from_gutenberg():
    """Download books from Project Gutenberg"""
    print("\n" + "="*50)
    print("DOWNLOADING FROM PROJECT GUTENBERG")
    print("="*50 + "\n")
    
    for book in GUTENBERG_BOOKS:
        print(f"--- {book['title']} ---")
        
        text_path = DATA_DIR / "texts" / f"{book['id']}.txt"
        
        if text_path.exists():
            print(f"  âœ“ Already exists: {text_path.name}")
        else:
            text = download_text_from_url(book['url'], use_headers=True)
            if text:
                # Clean Gutenberg header/footer
                text = clean_gutenberg_text(text)
                with open(text_path, 'w', encoding='utf-8') as f:
                    f.write(text)
                print(f"  âœ“ Saved: {text_path.name} ({len(text)} chars)")
                
                # Save metadata
                save_metadata(book['id'], book['metadata'])
            
            time.sleep(1)
        print()


def download_from_archive_org_api():
    """Download texts from Internet Archive using their API"""
    print("\n" + "="*50)
    print("DOWNLOADING FROM INTERNET ARCHIVE (API)")
    print("="*50 + "\n")
    
    for item_id in ARCHIVE_ORG_TEXTS:
        print(f"--- {item_id} ---")
        
        text_path = DATA_DIR / "texts" / f"archive_{item_id}.txt"
        
        if text_path.exists():
            print(f"  âœ“ Already exists")
            continue
        
        try:
            # Get item metadata
            metadata_url = f"https://archive.org/metadata/{item_id}"
            response = requests.get(metadata_url, headers=HEADERS, timeout=20)
            
            if response.status_code == 200:
                data = response.json()
                
                # Find text file
                text_file = None
                if 'files' in data:
                    for f in data['files']:
                        name = f.get('name', '')
                        if name.endswith('_djvu.txt') or name.endswith('.txt'):
                            text_file = name
                            break
                
                if text_file:
                    text_url = f"https://archive.org/download/{item_id}/{text_file}"
                    text = download_text_from_url(text_url)
                    
                    if text and len(text) > 100:
                        with open(text_path, 'w', encoding='utf-8') as f:
                            f.write(clean_text(text))
                        print(f"  âœ“ Downloaded text file ({len(text)} chars)")
                        
                        save_metadata(f"archive_{item_id}", {
                            "source": "Internet Archive",
                            "item_id": item_id,
                            "type": "historical",
                            "language": "EN"
                        })
                    else:
                        print(f"  âš  Text file too short or empty")
                else:
                    print(f"  âš  No text file found in item")
            else:
                print(f"  âœ— API returned {response.status_code}")
        
        except Exception as e:
            print(f"  âœ— Error: {str(e)[:80]}")
        
        time.sleep(2)
    
    print()


def scrape_wikipedia_tunisia():
    """Scrape Wikipedia pages using API"""
    print("\n" + "="*50)
    print("DOWNLOADING FROM WIKIPEDIA")
    print("="*50 + "\n")
    
    api_url = "https://en.wikipedia.org/w/api.php"
    
    wikipedia_pages = [
        "Tunisian_independence",
        "History_of_Tunisia",
        "Culture_of_Tunisia",
        "French_protectorate_of_Tunisia",
        "Tunisian_revolution",
        "Habib_Bourguiba",
        "Carthage",
        "Berbers"
    ]
    
    for page_name in wikipedia_pages:
        print(f"Fetching: {page_name}")
        
        text_file = DATA_DIR / "texts" / f"wikipedia_{page_name}.txt"
        
        if text_file.exists():
            print(f"  âœ“ Already exists")
            continue
        
        try:
            params = {
                'action': 'query',
                'prop': 'extracts',
                'explaintext': True,
                'titles': page_name.replace('_', ' '),
                'format': 'json',
                'formatversion': '2'
            }
            
            response = requests.get(api_url, params=params, headers=HEADERS, timeout=20)
            data = response.json()
            
            if 'query' in data and 'pages' in data['query']:
                page = data['query']['pages'][0]
                text = page.get('extract', '')
                
                if text and len(text) > 500:
                    with open(text_file, 'w', encoding='utf-8') as f:
                        f.write(f"Title: {page.get('title', page_name)}\n\n{text}")
                    
                    print(f"  âœ“ Saved ({len(text)} chars)")
                    
                    metadata = {
                        "source": "Wikipedia",
                        "page": page_name,
                        "type": "encyclopedia",
                        "language": "EN",
                        "location": "Tunisia"
                    }
                    save_metadata(f"wikipedia_{page_name}", metadata)
                else:
                    print(f"  âš  Text too short or empty")
            else:
                print(f"  âœ— No pages found in response")
        
        except Exception as e:
            print(f"  âœ— Error: {str(e)[:80]}")
        
        time.sleep(1)
    
    print()


def create_sample_arabic_stories():
    """Create sample Arabic & French stories"""
    print("\n" + "="*50)
    print("CREATING SAMPLE ARABIC & FRENCH STORIES")
    print("="*50 + "\n")
    
    sample_stories = [
        {
            "id": "sample_resistance_sousse",
            "text": """Ù‚ØµØ© Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© ÙÙŠ Ø³ÙˆØ³Ø© - Ù¡Ù©Ù¥Ù¢

ÙÙŠ Ø¹Ø§Ù… Ù¡Ù©Ù¥Ù¢ØŒ Ù‚Ø§Ø¯ Ù…Ø­Ù…Ø¯ Ø§Ù„Ø²ÙˆØ§Ø±ÙŠØŒ Ø§Ù„Ø´Ù‡ÙŠØ¯ Ø§Ù„Ø¨Ø·Ù„ØŒ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…ÙŠÙ† ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø³ÙˆØ³Ø© Ø¶Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù…Ø§Ø± Ø§Ù„ÙØ±Ù†Ø³ÙŠ. ÙƒØ§Ù† Ø§Ù„Ø²ÙˆØ§Ø±ÙŠ Ø±Ø¬Ù„Ø§Ù‹ Ø´Ø¬Ø§Ø¹Ø§Ù‹ Ù…Ù† Ø£Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©ØŒ Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙƒØªØ§Ø¬Ø± Ø¨Ø³ÙŠØ· Ù„ÙƒÙ†Ù‡ Ø±ÙØ¶ Ø§Ù„Ø®Ø¶ÙˆØ¹ Ù„Ù„Ø¸Ù„Ù….

Ù†Ø¸Ù… Ø§Ù„Ø²ÙˆØ§Ø±ÙŠ ÙˆØ±ÙØ§Ù‚Ù‡ Ù‡Ø¬Ù…Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù…ÙˆØ§ Ù…Ø¹Ø±ÙØªÙ‡Ù… Ø¨Ø£Ø²Ù‚Ø© Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù„Ù‡Ø±ÙˆØ¨ Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù…Ø§Ø±ÙŠØ©. ÙƒØ§Ù†Øª Ø§Ù„Ù†Ø³Ø§Ø¡ ÙŠØ³Ø§Ø¹Ø¯Ù† Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…ÙŠÙ† Ø¨Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø£Ø³Ù„Ø­Ø© ÙˆÙ†Ù‚Ù„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„.

ÙÙŠ Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ù„ÙŠØ§Ù„ÙŠØŒ Ø­Ø§ØµØ±Øª Ø§Ù„Ù‚ÙˆØ§Øª Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© Ù…Ù†Ø²Ù„ Ø§Ù„Ø²ÙˆØ§Ø±ÙŠ. Ù‚Ø§ØªÙ„ Ø¨Ø¨Ø³Ø§Ù„Ø© Ø­ØªÙ‰ Ø¢Ø®Ø± Ø±Ù…Ù‚ØŒ ÙˆØ±ÙØ¶ Ø§Ù„Ø§Ø³ØªØ³Ù„Ø§Ù…. Ø§Ø³ØªØ´Ù‡Ø¯ ÙÙŠ ØªÙ„Ùƒ Ø§Ù„Ù„ÙŠÙ„Ø©ØŒ Ù„ÙƒÙ† Ù‚ØµØªÙ‡ Ø¨Ù‚ÙŠØª Ø­ÙŠØ© ÙÙŠ Ø°Ø§ÙƒØ±Ø© Ø£Ù‡Ù„ Ø³ÙˆØ³Ø©.

ÙŠØ±ÙˆÙŠ ÙƒØ¨Ø§Ø± Ø§Ù„Ø³Ù† Ø£Ù† Ø±ÙˆØ­ Ø§Ù„Ø²ÙˆØ§Ø±ÙŠ Ø¸Ù„Øª ØªÙ„Ù‡Ù… Ø§Ù„Ø´Ø¨Ø§Ø¨ Ø­ØªÙ‰ Ù†Ø§Ù„Øª ØªÙˆÙ†Ø³ Ø§Ø³ØªÙ‚Ù„Ø§Ù„Ù‡Ø§ ÙÙŠ Ù¡Ù©Ù¥Ù¦.
""",
            "metadata": {
                "type": "resistance_story",
                "era": "colonial_1952",
                "language": "AR",
                "location": "Sousse",
                "story_type": "martyr_legend",
                "source": "oral_tradition"
            }
        },
        {
            "id": "sample_french_resistance_1943",
            "text": """La RÃ©sistance Tunisienne - 1943

RÃ©cit de rÃ©sistance Ã  Bizerte

Pendant la Seconde Guerre mondiale, quand les forces allemandes occupÃ¨rent la Tunisie en 1942-1943, de nombreux Tunisiens rejoignirent la rÃ©sistance contre l'occupation.

Ali Ben Salem, un pÃªcheur de Bizerte, utilisait son bateau pour aider les rÃ©sistants franÃ§ais et tunisiens. Il transportait des messages, des armes et parfois des personnes recherchÃ©es par les Allemands.

Une nuit de fÃ©vrier 1943, Ali reÃ§ut une mission dangereuse : aider trois pilotes britanniques dont l'avion s'Ã©tait Ã©crasÃ© prÃ¨s de la cÃ´te. MalgrÃ© les patrouilles allemandes, Ali navigua dans l'obscuritÃ© et sauva les trois hommes.

Les Allemands soupÃ§onnÃ¨rent Ali et fouillÃ¨rent son bateau plusieurs fois, mais ne trouvÃ¨rent jamais de preuves. AprÃ¨s la libÃ©ration de la Tunisie en mai 1943, Ali continua sa vie de pÃªcheur, rarement parlant de ses actes hÃ©roÃ¯ques.

Ses petits-enfants racontent aujourd'hui comment leur grand-pÃ¨re aidait tout le monde, sans distinction de nationalitÃ© ou de religion.
""",
            "metadata": {
                "type": "resistance_story",
                "era": "wwii_1943",
                "language": "FR",
                "location": "Bizerte",
                "story_type": "war_hero",
                "source": "oral_tradition"
            }
        }
    ]
    
    for story in sample_stories:
        text_file = DATA_DIR / "texts" / f"{story['id']}.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(story['text'])
        
        save_metadata(story['id'], story['metadata'])
        print(f"  âœ“ Created: {story['id']}")
    
    print()


def process_local_pdfs():
    """Process any PDFs manually placed in the pdfs folder"""
    print("\n" + "="*50)
    print("PROCESSING LOCAL PDFs")
    print("="*50 + "\n")
    
    pdf_files = list((DATA_DIR / "pdfs").glob("*.pdf"))
    
    if not pdf_files:
        print("  â„¹ No PDF files found in pdfs folder")
        print()
        return
    
    for pdf_path in pdf_files:
        text_path = DATA_DIR / "texts" / f"{pdf_path.stem}.txt"
        
        if text_path.exists():
            print(f"  âœ“ Already processed: {pdf_path.name}")
            continue
        
        print(f"Processing: {pdf_path.name}")
        text = extract_text_from_pdf(pdf_path)
        
        if text and len(text) > 100:
            cleaned_text = clean_text(text)
            with open(text_path, 'w', encoding='utf-8') as f:
                f.write(cleaned_text)
            print(f"  âœ“ Extracted text ({len(cleaned_text)} chars)")
            
            save_metadata(pdf_path.stem, {
                "source": "local_pdf",
                "filename": pdf_path.name,
                "type": "document",
                "language": "unknown"
            })
        else:
            print(f"  âš  Could not extract text or file too short")
    
    print()


def generate_dataset_summary():
    """Generate a summary of collected data"""
    print("\n" + "="*50)
    print("DATASET SUMMARY")
    print("="*50 + "\n")
    
    texts = list((DATA_DIR / "texts").glob("*.txt"))
    num_texts = len(texts)
    
    if num_texts > 0:
        total_chars = sum(len(open(f, 'r', encoding='utf-8').read()) for f in texts)
        print(f"ðŸ“„ Total texts: {num_texts}")
        print(f"ðŸ’¾ Total characters: {total_chars:,}")
        print(f"\nðŸ“š Downloaded texts:")
        for text_file in sorted(texts):
            size_kb = text_file.stat().st_size / 1024
            print(f"  â€¢ {text_file.name} ({size_kb:.1f} KB)")
    else:
        print("âš  No texts downloaded yet")
    
    print()


def main():
    """Main execution function"""
    print("\n" + "="*60)
    print("  TUNISIAN HERITAGE DATA COLLECTION SCRIPT v2.0")
    print("="*60)
    
    print("\nThis script will download from:")
    print("  â€¢ âœ… Project Gutenberg (public domain books)")
    print("  â€¢ âœ… Wikipedia API (encyclopedia articles)")
    print("  â€¢ âœ… Internet Archive API (digital library)")
    print("  â€¢ âœ… Sample stories (Arabic & French)")
    print("\nData will be saved to:", DATA_DIR.absolute())
    print()
    
    input("Press Enter to start downloading...")
    
    # Run all collection functions
    download_from_gutenberg()
    scrape_wikipedia_tunisia()
    download_from_archive_org_api()
    create_sample_arabic_stories()
    process_local_pdfs()
    
    # Generate summary
    generate_dataset_summary()
    
    print("\n" + "="*60)
    print("  âœ“ DATA COLLECTION COMPLETE!")
    print("="*60)
    print(f"\nAll data saved to: {DATA_DIR.absolute()}")
    print("\nNext steps:")
    print("  1. Review downloaded texts in 'texts' folder")
    print("  2. Check metadata in 'metadata' folder") 
    print("  3. Use these texts for your RAG system ingestion")
    print()
    
    # Show statistics
    texts = list((DATA_DIR / "texts").glob("*.txt"))
    if texts:
        total_chars = sum(len(open(f, 'r', encoding='utf-8').read()) for f in texts)
        print(f"ðŸ“Š Downloaded {len(texts)} texts with {total_chars:,} total characters")
    print()


if __name__ == "__main__":
    main()
